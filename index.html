<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Moving Off-the-Grid: Scene-Grounded Video Representations">
  <meta name="keywords" content="MooG">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Moving Off-the-Grid: Scene-Grounded Video Representations</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h2 class="title is-2 publication-title">Moving Off-the-Grid: Scene-Grounded Video Representations</h2>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://www.sjoerdvansteenkiste.com/">Sjoerd van Steenkiste</a><sup>1*</sup>,
              </span>
              <span class="author-block">
                <a href="https://people.csail.mit.edu/danielzoran/">Daniel Zoran</a><sup>2*</sup>,
              </span>
              <span class="author-block">
                <a href="https://yangyi02.github.io">Yi Yang</a><sup>2</sup>,
              </span>              
              <span class="author-block">
                <a href="https://yuliarubanova.github.io/">Yulia Rubanova</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=NVD-BU4AAAAJ">Rishabh Kabra</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="http://www.carldoersch.com">Carl Doersch</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=cnbENAEAAAAJ">Dilara Gokay</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=kbqjyGQAAAAJ">Joseph Heyward</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="http://e-pot.xyz/">Etienne Pot</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://qwlouse.github.io/">Klaus Greff</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://cs.stanford.edu/~dorarad/">Drew A. Hudson</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.co.uk/citations?user=GRICvzoAAAAJ">Thomas Albert Keck</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=IUZ-7_cAAAAJ">Joao Carreira</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.de/citations?user=FXNJRDoAAAAJ">Alexey Dosovitskiy</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="https://msajjadi.com/">Mehdi S. M. Sajjadi</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://tkipf.github.io/">Thomas Kipf</a><sup>2*</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Google Research,</span>
              <span class="author-block"><sup>2</sup>Google DeepMind,</span>
              <span class="author-block"><sup>3</sup>Inceptive</span>
            </div>

            <div class="is-size-7">
              <span class="author-block">(*: equal contribution)</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://openreview.net/pdf?id=rjSPDVdUaw"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2411.05927" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href=""
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming Soon)</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <img src="static/images/teaser_v1.png" alt="Description of Image">
            <p>
              MooG is a recurrent, transformer-based, video representation model that can be unrolled
              through time. MooG learns a set of “off-the-grid” latent representation.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h3 class="title is-3">Abstract</h3>
          <div class="content has-text-justified">
            <p>
              Current vision models typically maintain a fixed correspondence between their representation structure and image space. Each layer comprises a set of tokens arranged “on-the-grid,” which biases patches or tokens to encode information at a specific spatio(-temporal) location. In this work we present Moving Off-the-Grid (MooG), a self-supervised video representation model that offers an alternative approach, allowing tokens to move “off-the-grid” to better enable them to represent scene elements consistently, even as they move across the image plane through time. By using a combination of cross-attention and positional embeddings we disentangle the representation structure and image structure. We find that a simple self-supervised objective—next frame prediction—trained on video data, results in a set of latent tokens which bind to specific scene structures and track them as they move. We demonstrate the usefulness of MooG’s learned representation both qualitatively and quantitatively by training readouts on top of the learned representation on a variety of downstream tasks. We show that MooG can provide a strong foundation for different vision tasks when compared to “on-the-grid” baselines.
            </p>
          </div>
        </div>
      </div>

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h3 class="title is-3">Video Summary</h3>
          <a href="https://recorder-v3.slideslive.com/?share=95107&s=f5e93695-de87-4b92-99a5-96eec81bf67e">
            <img src="static/images/neurips.png" alt="Video Summary">
          </a>
        </div>
      </div>
    </div>

    <div class="container is-max-desktop">

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content">

            <h3 class="title is-3">Visualizing Token Attention Maps</h3>
            <div class="container">
              <p>
                For each pixel location, at each frame, we colour code the token that has the most attention weight at that location. If the representation is stable - i.e. if the same token tracks the same content as it moves - we should see the motion of the token argmax move with the scene motion, which is the case.
              </p>
              <div id="results-carousel" class="carousel results-carousel">
                <!-- First item in the combined carousel -->
                <div class="item">
                  <div class="image-container">
                    <img src="supplementary/example.gif" alt="Decoder Arg-Max Attention Map" style="width: 100%; height: auto;">
                  </div>
                  <div class="token-row">
                    <img src="supplementary/example_token_1.gif" alt="Individual Token Attention Map 1" style="width: 25%; height: auto;">
                    <img src="supplementary/example_token_2.gif" alt="Individual Token Attention Map 2" style="width: 25%; height: auto;">
                    <img src="supplementary/example_token_3.gif" alt="Individual Token Attention Map 3" style="width: 25%; height: auto;">
                    <img src="supplementary/example_token_4.gif" alt="Individual Token Attention Map 4" style="width: 25%; height: auto;">
                  </div>
                </div>
            
                <!-- Additional items -->
                <div class="item">
                  <div class="image-container">
                    <img src="supplementary/more_examples/example_1.gif" alt="Decoder Arg-Max Attention Map" style="width: 100%; height: auto;">
                  </div>
                  <div class="token-row">
                    <img src="supplementary/more_examples/example_1_1.gif" alt="Individual Token Attention Map 1" style="width: 25%; height: auto;">
                    <img src="supplementary/more_examples/example_1_2.gif" alt="Individual Token Attention Map 2" style="width: 25%; height: auto;">
                    <img src="supplementary/more_examples/example_1_3.gif" alt="Individual Token Attention Map 3" style="width: 25%; height: auto;">
                    <img src="supplementary/more_examples/example_1_4.gif" alt="Individual Token Attention Map 4" style="width: 25%; height: auto;">
                  </div>
                </div>
            
                <!-- Repeat the structure for other items -->
                <div class="item">
                  <div class="image-container">
                    <img src="supplementary/more_examples/example_2.gif" alt="Decoder Arg-Max Attention Map" style="width: 100%; height: auto;">
                  </div>
                  <div class="token-row">
                    <img src="supplementary/more_examples/example_2_1.gif" alt="Individual Token Attention Map 1" style="width: 25%; height: auto;">
                    <img src="supplementary/more_examples/example_2_2.gif" alt="Individual Token Attention Map 2" style="width: 25%; height: auto;">
                    <img src="supplementary/more_examples/example_2_3.gif" alt="Individual Token Attention Map 3" style="width: 25%; height: auto;">
                    <img src="supplementary/more_examples/example_2_4.gif" alt="Individual Token Attention Map 4" style="width: 25%; height: auto;">
                  </div>
                </div>
            
                <!-- Repeat the structure for other items -->
                <div class="item">
                  <div class="image-container">
                    <img src="supplementary/more_examples/example_3.gif" alt="Decoder Arg-Max Attention Map" style="width: 100%; height: auto;">
                  </div>
                  <div class="token-row">
                    <img src="supplementary/more_examples/example_3_1.gif" alt="Individual Token Attention Map 1" style="width: 25%; height: auto;">
                    <img src="supplementary/more_examples/example_3_2.gif" alt="Individual Token Attention Map 2" style="width: 25%; height: auto;">
                    <img src="supplementary/more_examples/example_3_3.gif" alt="Individual Token Attention Map 3" style="width: 25%; height: auto;">
                    <img src="supplementary/more_examples/example_3_4.gif" alt="Individual Token Attention Map 4" style="width: 25%; height: auto;">
                  </div>
                </div>

                <!-- Repeat the structure for other items -->
                <div class="item">
                  <div class="image-container">
                    <img src="supplementary/more_examples/example_4.gif" alt="Decoder Arg-Max Attention Map" style="width: 100%; height: auto;">
                  </div>
                  <div class="token-row">
                    <img src="supplementary/more_examples/example_4_1.gif" alt="Individual Token Attention Map 1" style="width: 25%; height: auto;">
                    <img src="supplementary/more_examples/example_4_2.gif" alt="Individual Token Attention Map 2" style="width: 25%; height: auto;">
                    <img src="supplementary/more_examples/example_4_3.gif" alt="Individual Token Attention Map 3" style="width: 25%; height: auto;">
                    <img src="supplementary/more_examples/example_4_4.gif" alt="Individual Token Attention Map 4" style="width: 25%; height: auto;">
                  </div>
                </div>
                
              </div>
              <p>
                Top Row from left to right: colour coded arg-max tokens, blended arg-max with video, ground truth video, frame predictions. Bottom row: 4 randomly selected tokens latch to specific scene elements and track them as they move
              </p>
            </div>

            <h3 class="title is-3">PCA Analysis of Tokens</h3>
            <div class="container">
              <p>
                We unroll the model over a batch of 24 short clips, each 12 frames in length. We take the predicted states of all clips across all time steps to obtain a 294912 x 512 matrix where 512 is the token size. We calculate the PCA components of this matrix and take 3 (out of 512) of the leading components and visualize them as RGB. Note the consistent cross sequence structure, relating to meaningful elements in the scene. From left to right, PCA in RGB, blended with original video, original video.
              </p>
              <div id="results-carousel" class="carousel results-carousel">
                <div class="item">
                  <img src="supplementary/pca/pca_1.gif" alt="Description of Image" style="width: 100%; height: 100%;">
                </div>
                <div class="item">
                  <img src="supplementary/pca/pca_2.gif" alt="Description of Image" style="width: 100%; height: 100%;">
                </div>
                <div class="item">
                  <img src="supplementary/pca/pca_3.gif" alt="Description of Image" style="width: 100%; height: 100%;">
                </div>
                <div class="item">
                  <img src="supplementary/pca/pca_4.gif" alt="Description of Image" style="width: 100%; height: 100%;">
                </div>
                <div class="item">
                  <img src="supplementary/pca/pca_5.gif" alt="Description of Image" style="width: 100%; height: 100%;">
                </div>
              </div>
            </div>

            <h3 class="title is-3">Changing the Number of Tokens</h3>
            <div class="container">
              <p>
                Since the model has a latent set of tokens there are no parameters in the model that depends on the number of tokens. As a result we can instantiate the model with a varying number of tokens without retraining. As can be seen the model adapts elegantly, making the tokens bind to larger areas of the image but still being able to predict future frames adequatly and track scene structure. This model has been trained with 1024 tokens. Shown, from top to bottom, are 256, 512 and 1024 tokens, from right to left predictions, ground truth frames, blended argmax attention, argmax attention.
              </p>
              <div class="item">
                <img src="supplementary/test_number_of_tokens/256_tokens.gif" alt="Description of Image" style="width: 100%; height: 100%;">
              </div>
              <div class="item">
                <img src="supplementary/test_number_of_tokens/512_tokens.gif" alt="Description of Image" style="width: 100%; height: 100%;">
              </div>
              <div class="item">
                <img src="supplementary/test_number_of_tokens/1024_tokens.gif" alt="Description of Image" style="width: 100%; height: 100%;">
              </div>
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h3 class="title is-3">Related Links</h3>
          <div class="content has-text-justified">
            <p>
              <a href="https://slot-attention-video.github.io/">SAVi: Conditional Object-Centric Learning from Video</a> encodes a video into a set of temporally-consistent latent variables (object slots), and is trained to predict optical flow or to reconstruct input frames.
            </p>
            <p>
              <a href="https://slot-attention-video.github.io/savi++/">SAVi++: Towards End-to-End Object-Centric Learning from Real-World Videos</a> improves SAVi by utilizing depth prediction and by adopting best practices for model scaling
              in terms of architecture design and data augmentation.
            </p>
            <p>
              <a href="https://srt-paper.github.io/">Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations</a> takes few posed or unposed images of novel real-world scenes as input and produces a set-latent scene representation that is decoded to 3D videos & semantics.
            </p>            
            <p>
              <a href="https://rust-paper.github.io/">RUST: Latent Neural Scene Representations from Unposed Imagery</a> learns a latent pose space through self supervision by taking a peek at the target view during training.
            </p>
            <p>
              <a href="https://dyst-paper.github.io/">DyST: Towards Dynamic Neural Scene Representations on Real-World Videos</a> learns tangible latent representations for dynamic scenes that enable view generation with separate control over the camera and the content of the scene.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://arxiv.org/abs/2211.03726">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/deepmind/tapnet" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/moog-paper/moog-paper.github.io">source code</a> of this website, which itelf is a fork of <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. We just ask that you link back to this page in the footer.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
